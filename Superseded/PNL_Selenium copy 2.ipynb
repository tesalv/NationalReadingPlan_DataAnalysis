{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a library that we will use in order for creating break times in order to mimic the human behaviour \n",
    "import time\n",
    "\n",
    "#this library will be used for getting your credential's and don't be shown in the scrip \n",
    "from getpass import getpass\n",
    "\n",
    "#this are the libraries we will be using for interacting with a webpage\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "\n",
    "# library to open interact with operating system\n",
    "import os\n",
    "\n",
    "# You know pandas it's your best buddy \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "        # opening wesite\n",
    "driver.get('https://www.pnl2027.gov.pt/np4/livrospnl?cat_livrospnl=catalogo_blx#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists of potential parameters\n",
    "\n",
    "age_list=[\"0 - 2 anos\",\"3 - 5 anos\",\"6 - 8 anos\",\"9 - 11 anos\",\"12 - 14 anos\",\"15 - 18 anos\",\"maiores de 18 anos\"]\n",
    "\n",
    "theme_list=[\"Arte\",\"Banda Desenhada\",\"Biografia\",\"Ciência e Tecnologia\",\"Cultura e Sociedade\",\"Ensaio\",\"Literatura\",\"Poesia\",\"Vida Prática\"]\n",
    "\n",
    "period_list=[\"Antes 2017\",\"2017\",\"2018 - 1.º Sem.\",\"2018 - 2.º Sem.\",\"2019 - 1.º Sem.\",\"2019 - 2.o Sem.\",\"2020 - 1.º Sem.\",\"2020 - 2.o Sem.\",\"2021 - 1.o Sem.\",\"2021 - 2.o Sem.\",\"2022 - 1.o Sem.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting html from settings we want to look for in the page\n",
    "\n",
    "def parameters_to_select(driver,age,theme,period):\n",
    "\n",
    "    select_age =Select(driver.find_element(By.ID,'limitbox_1'))\n",
    "    select_age.select_by_visible_text(age)\n",
    "    time.sleep(random.randint(1,2))\n",
    "\n",
    "    #select theme\n",
    "    select_theme =Select(driver.find_element(By.ID,'limitbox_3'))\n",
    "    select_theme.select_by_visible_text(theme)\n",
    "    time.sleep(random.randint(1,2))\n",
    "\n",
    "    #select period\n",
    "    select_period =Select(driver.find_element(By.ID,'limitbox_6'))\n",
    "    select_period.select_by_visible_text(period)\n",
    "    time.sleep(random.randint(1,2))\n",
    "\n",
    "\n",
    "    #select search bar \n",
    "    #search_bar =Select(driver.find_element(By.CSS_SELECTOR, \"input[class^='atividade_filter']\"))\n",
    "    search_bar =driver.find_element(By.XPATH, '//*[@id=\"search_terms\"]')\n",
    "    search_bar.send_keys(Keys.ENTER)\n",
    "    time.sleep(12)\n",
    "\n",
    "    # get the page you are in using page_source attribute \n",
    "    html = driver.page_source\n",
    "    \n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_details(soup, res_pages):\n",
    "    book_titles=[]\n",
    "    book_author=[]\n",
    "    book_editor=[]\n",
    "    page_result=[]\n",
    "    location=2\n",
    "    for page in res_pages:\n",
    "\n",
    "        ## get the book details\n",
    "\n",
    "        tables = soup.find_all(\"div\",attrs={\"class\":\"newsDetailBlx\"})\n",
    "\n",
    "        # book_titles.append([(i.find(\"div\",attrs={\"class\":\"book_title\"})).text for i in tables])\n",
    "        # book_author.append([(i.find(\"div\",attrs={\"class\":\"book_author\"})).text for i in tables])\n",
    "        # book_editor.append([(i.find(\"div\",attrs={\"class\":\"book_editor\"})).text for i in tables])\n",
    "\n",
    "        for i in tables:\n",
    "            book_titles.append(i.find(\"div\",attrs={\"class\":\"book_title\"}).text)\n",
    "            book_author.append(i.find(\"div\",attrs={\"class\":\"book_author\"}).text)\n",
    "            book_editor.append(i.find(\"div\",attrs={\"class\":\"book_editor\"}).text)\n",
    "            page_result.append(page)\n",
    "\n",
    "        ## go to next page\n",
    "        #page_nr=int(page)\n",
    "        if location>int(res_pages[-1]):\n",
    "            break\n",
    "        else:\n",
    "            select_page =driver.find_element(By.CSS_SELECTOR,f\"a[onclick^='event.preventDefault();changePage({location});']\")\n",
    "            select_page.click()\n",
    "            time.sleep(10)\n",
    "            #print(page)\n",
    "            \n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html)\n",
    "            location +=1\n",
    "            #time.sleep(5)\n",
    "        \n",
    "    #create database with all the info\n",
    "    PNL=pd.DataFrame(list(zip(book_titles,book_author,book_editor,page_result)),columns=[\"title\",\"author\",\"publisher\",\"res_page\"])\n",
    "    PNL[\"age_recom\"]=age\n",
    "    PNL[\"theme\"]=theme\n",
    "    PNL[\"PNLedition\"]=period\n",
    "    PNL[['1st_author',\"other_authors\"]] = PNL.author.str.split(';',n=1, expand=True,)\n",
    "    #PNL[\"results_page\"]=page\n",
    "    return PNL\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the database in csv file\n",
    "\n",
    "def export_csv(df):\n",
    "    \n",
    "    return df.to_csv(f\"datasets\\PNL_{age}_{period}.csv\",encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters we want to export data from\n",
    "\n",
    "#age_list=[\"0 - 2 anos\",\"3 - 5 anos\",\"6 - 8 anos\",\"9 - 11 anos\",\"12 - 14 anos\",\"15 - 18 anos\",\"maiores de 18 anos\"]\n",
    "age='15 - 18 anos'\n",
    "\n",
    "#theme_list=[\"Arte\",\"Banda Desenhada\",\"Biografia\",\"Ciência e Tecnologia\",\"Cultura e Sociedade\",\"Ensaio\",\"Literatura\",\"Poesia\",\"Vida Prática\"]\n",
    "theme='Literatura'\n",
    "\n",
    "#period_list=[\"Antes 2017\",\"2017\",\"2018 - 1.º Sem.\",\"2018 - 2.º Sem.\",\"2019 - 1.º Sem.\",\"2019 - 2.o Sem.\",\"2020 - 1.º Sem.\",\"2020 - 2.o Sem.\",\"2021 - 1.o Sem.\",\"2021 - 2.o Sem.\",\"2022 - 1.o Sem.\"]\n",
    "period='Antes 2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=parameters_to_select(driver,age,theme,period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ##use beautiful soup to extract the data we want\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "## get the number of result pages\n",
    "\n",
    "nr_pages=soup.find_all(\"a\",attrs={\"href\":\"./?\"})\n",
    "#display(nr_pages)\n",
    "nr_pages=list(set([i.text for i in nr_pages]))\n",
    "nr_pages.sort()\n",
    "\n",
    "if \".\" in nr_pages[0]:\n",
    "    ref_number=int(nr_pages[0].strip(\"...\"))\n",
    "else:\n",
    "    ref_number=int(nr_pages[-1])\n",
    "    \n",
    "#print(nr_pages[0].strip(\"...\"))\n",
    "print(ref_number)\n",
    "print(list(range(1,ref_number+1)))\n",
    "#nr_pages[0].find(\"onclick\")\n",
    "book_titles=[]\n",
    "book_author=[]\n",
    "book_editor=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "PNL_df=get_book_details(soup,nr_pages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv(PNL_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ironhack')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a38339ccb09af62c11572ee1f8d8bf4c38a6a8e49e8be3ae680a571cc75115c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
